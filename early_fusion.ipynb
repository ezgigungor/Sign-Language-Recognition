{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "early_fusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuzTFebaJ55_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, dataset, random_split\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab.patches import cv2_imshow\n",
        "import collections\n",
        "from PIL import ImageFile\n",
        "import torch.nn.functional as F\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zjxp1IMJ6gI"
      },
      "source": [
        "classes = 'drive/My Drive/MS-ASL/MSASL_classes.json'\n",
        "train_json = 'drive/My Drive/MS-ASL/MSASL_train.json'\n",
        "test_json = 'drive/My Drive/MS-ASL/MSASL_test.json'\n",
        "val_json = 'drive/My Drive/MS-ASL/MSASL_val.json'\n",
        "train_videos = 'drive/My Drive/HW4_DATA/videos/train/'\n",
        "val_videos = 'drive/My Drive/HW4_DATA/videos/val/'\n",
        "test_videos = 'drive/My Drive/HW4_DATA/videos/test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n76IRgBUJ6io"
      },
      "source": [
        "classes = {'hello' : 0, 'nice' :1, 'teacher':2,  'eat':3 , 'no':4, 'happy':5, 'like':6, 'orange':7, 'want' :8, 'deaf':9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJhYGRHXJ6k1"
      },
      "source": [
        "def load_data(path,is_test):\n",
        "  df = pd.read_json(path)\n",
        "  for value in df['clean_text'].values:\n",
        "    if value not in classes.keys():\n",
        "      df.drop(df[df['clean_text']== value].index, inplace = True)\n",
        "  df['url'] = df['url'].apply(lambda x : 'w' + x.lstrip('https://www.youtube.com'))\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvSuBR5fJ6nU"
      },
      "source": [
        "def clean_data(path, df):\n",
        "  files = [os.path.splitext(f)[0] for f in listdir(path) if isfile(join(path, f))]\n",
        "  with_ext = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "  org_values = df['url'].values\n",
        "  for value in df['url'].values:\n",
        "    if value not in files:\n",
        "      df.drop(df[df['url']== value].index, inplace = True) \n",
        "  \n",
        "\n",
        "  for i in range(0, df.shape[0]):\n",
        "    if df.iloc[i,12] in files:\n",
        "      index = files.index(df.iloc[i,12])\n",
        "      df = df.replace(df.iloc[i,12], with_ext[index])\n",
        " \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0AjRjqRJ6p3"
      },
      "source": [
        "train_df = load_data(train_json, False)\n",
        "train_df = clean_data(train_videos, train_df)\n",
        "val_df = load_data(val_json, False)\n",
        "val_df = clean_data(val_videos, val_df)\n",
        "test_df = load_data(test_json, True)\n",
        "test_df = clean_data(test_videos, test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZxzqJu4KPJg"
      },
      "source": [
        "class TemporalDataset(datasets.ImageFolder):\n",
        "\n",
        "  def __init__(self,path,df, classes, transform):\n",
        "    self.path = path\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "    path =  self.path + self.df.iloc[idx][12] + '_' +  self.df.iloc[idx][1] + \".npy\"\n",
        "\n",
        "    flow = np.load(path)\n",
        "    flow = np.transpose(flow)\n",
        "  \n",
        "\n",
        "\n",
        "    label = self.classes[self.df.iloc[idx][1]]\n",
        "\n",
        "    return  (flow, label) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyeRGuGJKPMY"
      },
      "source": [
        "class SpatialDataset(datasets.ImageFolder):\n",
        "\n",
        "    def __init__(self,path,df, classes, transform):\n",
        "      self.path = path\n",
        "      self.df = df\n",
        "      self.transform = transform\n",
        "      self.classes = classes\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      url = self.df.iloc[idx][12]\n",
        "      \n",
        "      video = cv2.VideoCapture(self.path + url)\n",
        "    \n",
        "      frame = self.df.iloc[idx][6]  - self.df.iloc[idx][5]\n",
        "      video.set(cv2.CAP_PROP_POS_FRAMES, self.df.iloc[idx][5])\n",
        "      \n",
        "      ret, image = video.read()\n",
        "      \n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "      label = self.classes[self.df.iloc[idx][1]]\n",
        "      \n",
        "      return  (image, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trrpYwxTKtGU"
      },
      "source": [
        "temporal_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),                        \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb6FiFhyKtIz"
      },
      "source": [
        "spatial_transform = transforms.Compose([\n",
        "  transforms.ToPILImage(),\n",
        "  transforms.Resize(256),\n",
        "  transforms.CenterCrop(224),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),                            \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj47xyX4Kw44"
      },
      "source": [
        "def getdataloader_sizes(net, batchsize): \n",
        "  if(net == 'temporal'):\n",
        "    train =  TemporalDataset( path = 'drive/My Drive/flows/train/', df = train_df, classes = classes, transform = temporal_transform)\n",
        "    validation = TemporalDataset( path = 'drive/My Drive/flows/val/', df = val_df,  classes = classes, transform = temporal_transform)\n",
        "    test = TemporalDataset( path = 'drive/My Drive/flows/test/', df = test_df,  classes = classes, transform = temporal_transform)\n",
        "  else:\n",
        "    train =  SpatialDataset( path = train_videos, df = train_df, classes = classes, transform = spatial_transform)\n",
        "    validation = SpatialDataset( path = val_videos, df = val_df,  classes = classes, transform = spatial_transform)\n",
        "    test = SpatialDataset( path = test_videos, df = test_df,  classes = classes, transform = spatial_transform)\n",
        "\n",
        "  \n",
        "  all_datasets = {'train' : train, 'validation' : validation, 'test' : test}\n",
        "\n",
        "  dataloaders = {x: torch.utils.data.DataLoader(all_datasets[x], batch_size=batchsize,\n",
        "                                               shuffle=True)\n",
        "                for x in ['train', 'validation' ,'test']}\n",
        "  dataset_sizes = {x: len(all_datasets[x]) for x in ['train', 'validation','test']}\n",
        "\n",
        "  class_names = list(classes.keys())\n",
        "  print(class_names)\n",
        "  print(dataset_sizes)\n",
        "  return dataloaders,dataset_sizes,class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q9ZMyn_Kw7b"
      },
      "source": [
        "class SpatialNet(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(SpatialNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 96, kernel_size=7, stride=2)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.pool1 = nn.MaxPool2d(3, stride=2)\n",
        "      self.norm1 = nn.LocalResponseNorm(2)\n",
        "      self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=2)\n",
        "      self.relu2 = nn.ReLU()\n",
        "      self.pool2 = nn.MaxPool2d(3, stride=2)\n",
        "      self.norm2 = nn.LocalResponseNorm(2)\n",
        "      self.conv3 = nn.Conv2d(256, 512, kernel_size=3)\n",
        "      self.relu3 = nn.ReLU()\n",
        "      self.conv4 = nn.Conv2d(512, 512, kernel_size=3)\n",
        "      self.relu4= nn.ReLU()\n",
        "      self.conv5 = nn.Conv2d(512, 512, kernel_size=3)       \n",
        "      self.relu5 = nn.ReLU()\n",
        "      self.pool3 =nn.MaxPool2d(3, stride=2)\n",
        "  \n",
        "  \n",
        "      self.l1 = nn.Linear(2048, 4096)\n",
        "      self.drop1 = nn.Dropout()\n",
        "      self.l2 = nn.Linear(4096, 2048)\n",
        "      self.drop2 = nn.Dropout()\n",
        "      self.l3 =nn.Linear(2048, 10)\n",
        "        \n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.relu1(x)\n",
        "      x = self.pool1(x)\n",
        "      x = self.norm1(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.relu2(x)\n",
        "      x = self.pool2(x)\n",
        "      x = self.norm2(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = self.relu3(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = self.relu4(x)\n",
        "\n",
        "\n",
        "      x = self.conv5(x)\n",
        "      x = self.relu5(x)\n",
        "      x = self.pool3(x)\n",
        "\n",
        "      x = x.view(x.size(0), -1)\n",
        "      \n",
        "      x = self.l1(x)\n",
        "      x = self.drop1(x)\n",
        "      x = self.l2(x)\n",
        "      x = self.drop2(x)\n",
        "      x = self.l3(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class TemporalNet(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(TemporalNet, self).__init__()\n",
        "      \n",
        "      self.conv1 = nn.Conv2d(10, 96, kernel_size=7, stride=2)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.pool1 = nn.MaxPool2d(3, stride=2)\n",
        "      self.norm1 = nn.LocalResponseNorm(2)\n",
        "      self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=2)\n",
        "      self.relu2 = nn.ReLU()\n",
        "      self.pool2 = nn.MaxPool2d(3, stride=2)\n",
        "      self.norm2 = nn.LocalResponseNorm(2)\n",
        "      self.conv3 = nn.Conv2d(256, 512, kernel_size=3)\n",
        "      self.relu3 = nn.ReLU()\n",
        "      self.conv4 = nn.Conv2d(512, 512, kernel_size=3)\n",
        "      self.relu4= nn.ReLU()\n",
        "      self.conv5 = nn.Conv2d(512, 512, kernel_size=3)           \n",
        "      self.relu5 = nn.ReLU()\n",
        "      self.pool3 =nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "  \n",
        "      self.l1 = nn.Linear(2048, 4096)\n",
        "      self.drop1 = nn.Dropout()\n",
        "      self.l2 = nn.Linear(4096, 2048)\n",
        "      self.drop2 = nn.Dropout()\n",
        "      self.l3 =nn.Linear(2048, 10)\n",
        "        \n",
        "      \n",
        "      \n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x.float())\n",
        "      x = self.relu1(x)\n",
        "      x = self.pool1(x)\n",
        "      x = self.norm1(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.relu2(x)\n",
        "      x = self.pool2(x)\n",
        "      x = self.norm2(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = self.relu3(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = self.relu4(x)\n",
        "\n",
        "\n",
        "      x = self.conv5(x)\n",
        "      x = self.relu5(x)\n",
        "      x = self.pool3(x)\n",
        "\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.l1(x)\n",
        "      x = self.drop1(x)\n",
        "      x = self.l2(x)\n",
        "      x = self.drop2(x)\n",
        "      x = self.l3(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "      \n",
        "spatial_net = SpatialNet()\n",
        "temporal_net = TemporalNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWbp_50CKw97"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPghyvS6LhQh"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    threshold = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j]),horizontalalignment=\"center\",color=\"white\" if cm[i, j] > threshold else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX9xeF-mLlCp"
      },
      "source": [
        "def plot_graph(plotlist1,plotlist2,ylabel):\n",
        "   \n",
        "    plt.xlabel(\"Training Epochs\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.plot(plotlist1, color=\"green\")\n",
        "    plt.plot(plotlist2, color=\"yellow\")\n",
        "    \n",
        "    plt.gca().legend(('Train', 'Validation'))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPi-4m7D78kn"
      },
      "source": [
        "class FusionNet(nn.Module):\n",
        "    def _init_(self, spatial_net, temporal_net):\n",
        "        super(FusionModel, self)._init_()\n",
        "        self.spatial = spatial_net\n",
        "        self.temporal = temporal_net\n",
        "        \n",
        "        self.modelA.fc3 = nn.Identity()\n",
        "        self.modelB.fc3 = nn.Identity()\n",
        "        \n",
        "        self.classifier = nn.Linear(2048+512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.spatial(x.clone())  \n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.temporal(x)\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        \n",
        "        x = self.classifier(F.relu(x))\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqWSgHBGLLT3"
      },
      "source": [
        "def train_model(model, criterion, optimizer, epoch_number,device,earlystopping):\n",
        "   \n",
        "    model.to(device)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_train_acc = 0.0\n",
        "    best_val_acc = 0.0\n",
        "    best_test_acc = 0.0\n",
        "    train_acc_history = list()\n",
        "    train_loss_history =list()\n",
        "    val_acc_history = list()\n",
        "    val_loss_history =list()\n",
        "    \n",
        "    counter = 0\n",
        "    stop =False\n",
        "    best_loss = None\n",
        "    \n",
        "   \n",
        "    n_epochs_stop = 1\n",
        "    min_val_loss = np.Inf\n",
        "    epochs_no_improve = 0\n",
        "    \n",
        "    for epoch in range(epoch_number):\n",
        "        if stop:\n",
        "          break\n",
        "        print('Epoch {}/{}'.format(epoch, epoch_number - 1))\n",
        "        \n",
        "        # Train and validation for each epoch\n",
        "        for part in ['train', 'validation']:\n",
        "            if part == 'train':\n",
        "                \n",
        "                model.train()  \n",
        "            else:\n",
        "                model.eval()  \n",
        "\n",
        "            current_loss = 0.0\n",
        "            current_phase_correct_outputnumber = 0\n",
        "            # For each phase in datasets are iterated\n",
        "            for inputs, labels in dataloaders[part]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(part == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # Backpropagate and opitimize Training part\n",
        "                    if part == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                current_loss += loss.item() * inputs.size(0)\n",
        "                current_phase_correct_outputnumber += torch.sum(preds == labels.data)\n",
        "\n",
        "            current_loss = current_loss / dataset_sizes[part]\n",
        "            epoch_acc = 100*current_phase_correct_outputnumber.double() / dataset_sizes[part]\n",
        "\n",
        "            if part == 'validation':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                val_loss_history.append(current_loss)\n",
        "                if earlystopping:\n",
        "                  # If the validation loss is at a minimum\n",
        "                  if current_loss < min_val_loss:\n",
        "                    # Save the model\n",
        "                    epochs_no_improve = 0\n",
        "                    min_val_loss = current_loss\n",
        "\n",
        "                  else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Check early stopping condition\n",
        "                    if epochs_no_improve == n_epochs_stop:\n",
        "                      print('Early stopping!')\n",
        "                      \n",
        "                      #Printed best accuracies\n",
        "                      print('Best train Acc: {:4f}'.format(best_train_acc))\n",
        "                      print('Best validation Acc: {:4f}'.format(best_val_acc))\n",
        "\n",
        "                      print()\n",
        "\n",
        "                      #Printed best accuracies\n",
        "                      print('Best train Acc: {:4f}'.format(best_train_acc))\n",
        "                      print('Best validation Acc: {:4f}'.format(best_val_acc))\n",
        "\n",
        "                      # load best model weights\n",
        "                      model.load_state_dict(best_model_wts)\n",
        "                      #Plot accuracy graph \n",
        "                      plot_graph(train_acc_history,val_acc_history,\"Accuracy\")\n",
        "                      plot_graph(train_loss_history,val_loss_history,\"Loss\")\n",
        "                      \n",
        "                      return model                  \n",
        "            else:\n",
        "                train_acc_history.append(epoch_acc)\n",
        "                train_loss_history.append(current_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                part, current_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if part == 'train' and epoch_acc > best_train_acc:\n",
        "                  best_train_acc = epoch_acc\n",
        "                \n",
        "            if part == 'validation' and epoch_acc > best_val_acc:             \n",
        "                best_val_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              \n",
        "        print() \n",
        "    \n",
        "              \n",
        "    print('Best train Acc: {:4f}'.format(best_train_acc))\n",
        "    print('Best validation Acc: {:4f}'.format(best_val_acc))\n",
        "    \n",
        "  \n",
        "    model.load_state_dict(best_model_wts)\n",
        "   \n",
        "    plot_graph(train_acc_history,val_acc_history,\"Accuracy\")\n",
        "    plot_graph(train_loss_history,val_loss_history,\"Loss\")\n",
        "  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1N8_8MRLLbk"
      },
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
        "\n",
        "spatial_model = spatial_net\n",
        "temporal_model = temporal_net\n",
        "\n",
        "\n",
        "#initializing parameters\n",
        "learning_rate = 0.01\n",
        "epoch = 32\n",
        "batchsize = 64\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "spatial_optimizer = torch.optim.SGD(spatial_model.parameters(), lr=learning_rate)\n",
        "temporal_optimizer = torch.optim.SGD(temporal_model.parameters(), lr=learning_rate)\n",
        "earlystoping = False\n",
        "\n",
        "\n",
        "dataloaders,dataset_sizes,class_names = getdataloader_sizes(batchsize)\n",
        "\n",
        "\n",
        "trained_spatial_model = train_model(spatial_model, criterion, spatial_optimizer,epoch,device,earlystoping)\n",
        "trained_temporal_model = train_model(temporal_model, criterion, temporal_optimizer,epoch,device,earlystoping)\n",
        "\n",
        "fusion_model = FusionNet(trained_spatial_model,trained_temporal_model)\n",
        "fusion_optimizer = torch.optim.SGD(fusion_model.parameters(), lr=learning_rate)\n",
        "trained_fusion_model = train_model(fuison_model, criterion, fusion_optimizer,epoch,device,earlystoping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZlyU0vcLLeL"
      },
      "source": [
        "def calculateTestAcc(trained_model,dataloaders,dataset_sizes):\n",
        "  confusion_matrixx = torch.zeros(10, 10)\n",
        "  np.set_printoptions(precision=2)\n",
        "  current_phase_correct_outputnumber = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
        "        inputs = inputs.to(device   )\n",
        "        classes = classes.to(device)\n",
        "        outputs = trained_model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        current_phase_correct_outputnumber += torch.sum(preds == classes.data)\n",
        "          \n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "            confusion_matrixx[t.long(), p.long()] += 1\n",
        "   \n",
        "    test_acc = 100*current_phase_correct_outputnumber.double() / dataset_sizes['test']\n",
        "\n",
        "    print('Test Acc: {:4f}'.format(test_acc))\n",
        "    \n",
        "  plt.figure(figsize = (10,10))\n",
        "  plot_confusion_matrix(confusion_matrixx,classes=class_names)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWxK1p-3MKVJ"
      },
      "source": [
        "calculateTestAcc(trained_fusion_model,dataloaders,dataset_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}